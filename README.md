# Hi there! 👋 I'm Ivan  
🚀 Computer Science student | 🤖 AI/ML Enthusiast

## 🏆 Featured Project: DiffusionGen  
DiffusionGen is a Flask-based GUI built for latent diffusion image generation, providing advanced user controls such as prompt input, image-to-image generation, and upscaling. The core diffusion model was implemented and trained entirely from scratch using PyTorch, without reliance on prebuilt diffusion libraries. The backend is designed for customization and extensibility, incorporating pretrained models—including Stable Diffusion’s VAE, CLIP for text embeddings, Real-ESRGAN for upscaling, and VGG16 for perceptual loss—into a tailored Latent Diffusion Model pipeline for improved image synthesis.

<br><br>

<p align="center">
  Prompt: The night sky stretches endlessly above a snow-covered winter landscape, shimmering with countless stars and the faint glow of the aurora borealis.
  <br><br>
  <img src="https://github.com/user-attachments/assets/ba885fd6-16c1-4ff2-9b36-a8e1f1414271" width="128" height="128">
</p>

<br>

🔗 **GitHub Repo**: [DiffusionGen](https://github.com/IvanC987/DiffusionGen)

### Other Projects:
🔗 **GitHub Repo**: [SemanticSegmentationModel](https://github.com/IvanC987/SemanticSegmentationModel) - Implements a U-Net model for semantic segmentation, designed to classify and segment objects within images at a pixel level. The model is trained using the Cityscapes dataset and optimized for accuracy in real-world segmentation tasks.

🔗 **GitHub Repo**: [TransformerLM](https://github.com/IvanC987/TransformerLM) - Implements a Transformer-based language model from scratch, integrating Byte Pair Encoding (BPE) for efficient tokenization. This project aims to provide a deeper understanding of how Transformers process and generate text at a more granular level.

🔗 **GitHub Repo**: [LanguageTranslationModel](https://github.com/IvanC987/LanguageTranslationModel) - Builds a sequence-to-sequence translation model based on the Transformer architecture. The project focuses on neural machine translation, using self-attention mechanisms for text generation.

---

## 🔧 Technologies & Tools I Use
![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazonaws&logoColor=white)
![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)

## 📫 How to Reach Me
- Email: IvanC135246@gmail.com  
- LinkedIn: [https://linkedin.com/in/IvanC987](https://www.linkedin.com/in/Ivan-Cao-CS/)

<br>

## 🔭 What I'm Working On
- Researching Transformer-based architectures
- Improving DiffusionGen with better UI/UX
- Learning model quantization, LoRA, and others for optimizing LLMs

<br>

### 🚀 Next Project: Revisiting `LangugageTranslationModel`
My next goal is to revisit my Translation Model project, focusing on using better dataset, enhancing accuracy, adding various adjustments




